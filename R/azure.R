#' @param host
#' @param securityToken
#' export
getAzureEndPoint <- function(host = "", securityToken = ""){
  AzureStor::storage_endpoint(endpoint = host, sas = securityToken)
}
#' @param host
#' @param securityToken
#' export
getAzureContainer <- function(host = "", securityToken = "", container = "") {
  endpoint <- getAzureEndPoint(host = host, securityToken = securityToken)
  AzureStor::storage_container(endpoint, container)
}
#' @param host
#' @param securityToken
#' export
listAzureContainers <- function(host = "", securityToken = ""){
  endpoint <- getAzureEndPoint(host = host, securityToken = securityToken)
  containers <- AzureStor::list_storage_containers(endpoint)
  df <- data.frame(matrix(unlist(containers), nrow = length(containers), byrow = TRUE))
  colnames(df) <- c("name", "endpoint", "sas", "version")
  df
}
#' @param host
#' @param securityToken
#' @param container
#' @param folder
#' export
listItemsInAzure <- function(host = "", securityToken = "", container = "", folder = ""){
  container <- getAzureContainer(host = host, securityToken = securityToken, container = container)
  AzureStor::list_storage_files(container, dir = folder, info = "all")
}
#' @param host
#' @param securityToken
#' @param container
#' @param folder
#' export
donwloadDataFileFromAzure <- function(host = "", securityToken = "", container = "", fileName = ""){
  shouldCacheFile <- getOption("tam.should.cache.datafile")
  filepath <- NULL
  hash <- digest::digest(stringr::str_c(host, container, fileName, sep = ":"), "md5", serialize = FALSE)
  tryCatch({
    filepath <- eval(as.name(hash))
  }, error = function(e){
    # if filePath hash is not set as global variable yet, it raises error that says object not found
    # which can be ignored
    filepath <- NULL
  })
  # Check if cached excel/csv exists for the filepath
  if (!is.null(shouldCacheFile) && isTRUE(shouldCacheFile) && !is.null(filepath)) {
    filepath
  } else {
    ext <- stringr::str_to_lower(tools::file_ext(fileName))
    tmp <- tempfile(fileext = stringr::str_c(".", ext))

    # In case of using Rserve on linux, somehow it doesn't create a temporary
    # directory specified by tempdir() which is used as a part of temp file
    # path generated by tempfile(). So if you try to use that temp file path,
    # dump some data into it for example, it will fail because no such path
    # found. This function fails with the same reason at download.file below.
    #
    # It works fine from the R command line on linux, and it works
    # fine all the time on Mac and Windows regardless Rserv or not.
    #
    # The following command is harmless even if you have the directory already.
    # http://stackoverflow.com/questions/4216753/check-existence-of-directory-and-create-if-doesnt-exist
    dir.create(tempdir(), showWarnings = FALSE)

    # download file to temporary location
    container <- exploratory::getAzureContainer(host = host, securityToken = securityToken, container = container)
    AzureStor::storage_download(container, src=fileName, dest = tmp, overwrite = T)
    # cache file
    if(!is.null(shouldCacheFile) && isTRUE(shouldCacheFile)){
      assign(hash, tmp, envir = .GlobalEnv)
    }
    tmp
  }
}

#'API that imports a CSV file from Azure.
#'@export
getCSVFileFromAzure <- function(fileName, host, securityToken, container, delim, quote = '"',
                             escape_backslash = FALSE, escape_double = TRUE,
                             col_names = TRUE, col_types = NULL,
                             locale = readr::default_locale(),
                             na = c("", "NA"), quoted_na = TRUE,
                             comment = "", trim_ws = FALSE,
                             skip = 0, n_max = Inf, guess_max = min(1000, n_max),
                             progress = interactive()) {
  tryCatch({
    filePath <- donwloadDataFileFromAzure(host = host, securityToken = securityToken, container = container, fileName = fileName)
  }, error = function(e) {
    if (stringr::str_detect(e$message, "(Not Found|Moved Permanently)")) {
      # Looking for error that looks like "Error in parse_aws_s3_response(r, Sig, verbose = verbose) :\n Moved Permanently (HTTP 301).",
      # or "Not Found (HTTP 404).".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-8 :: ', jsonlite::toJSON(c(bucket, fileName)), ' :: There is no such file in the Azure Container.'))
    }
    else {
      stop(e)
    }
  })
  exploratory::read_delim_file(filePath, delim = delim, quote = quote,
                               escape_backslash = escape_backslash, escape_double = escape_double,
                               col_names = col_names, col_types = col_types,
                               locale = locale,
                               na = na, quoted_na = quoted_na,
                               comment = comment, trim_ws = trim_ws,
                               skip = skip, n_max = n_max, guess_max = guess_max,
                               progress = progress)
}

getParquetFileFromAzure <- function(host = "", securityToken = "", container = "", fileName = "") {
  tryCatch({
    filePath <- donwloadDataFileFromAzure(host = host, securityToken = securityToken, container = container, fileName = fileName)
  }, error = function(e) {
    if (stringr::str_detect(e$message, "(Not Found|Moved Permanently)")) {
      # Looking for error that looks like "Error in parse_aws_s3_response(r, Sig, verbose = verbose) :\n Moved Permanently (HTTP 301).",
      # or "Not Found (HTTP 404).".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-8 :: ', jsonlite::toJSON(c(bucket, fileName)), ' :: There is no such file in the Azure Container.'))
    }
    else {
      stop(e)
    }
  })
  read_parquet(filePath)
}

tam_read_parquet.workaround_applied <- FALSE # To make sure we only apply the workaround explained below once.

read_parquet <- function(file) {
  loadNamespace("arrow")
  is.win <- Sys.info()['sysname'] == 'Windows'

  # Backup the locale info and set English locale for reading parquet issue
  # on Windows https://issues.apache.org/jira/browse/ARROW-7288
  # Also, we do it just once in an R session, since it seems to be enough,
  # and reqd_parquet under English locale has problem in reading from a path with multibyte chars,
  # which is unavoidable when repository is on Google Drive on Japanese Windows, even through a symbolic link.
  if (is.win && !tam_read_parquet.workaround_applied) {
    # Backup the current locale info.
    lc.all<- Sys.getlocale("LC_ALL")
    lc.collate<- Sys.getlocale("LC_COLLATE")
    lc.ctype<- Sys.getlocale("LC_CTYPE")
    lc.monetary<- Sys.getlocale("LC_MONETARY")
    lc.numeric<- Sys.getlocale("LC_NUMERIC")
    lc.time <- Sys.getlocale("LC_TIME")
    # Set English.
    Sys.setlocale("LC_ALL", "English")

    # Catch errors to guarantee restoring the locale info later.
    tryCatch({ # This tryCatch is just for restoring locale. Note that we do not catch errors here.
      tf0 <- tempfile()
      on.exit(unlink(tf0))
      df <- tibble::tibble(x=1)
      arrow::write_parquet(df, tf0, compression="uncompressed");
      arrow::read_parquet(tf0)
      tam_read_parquet.workaround_applied <<- TRUE
    }, finally = {
      # Restore the original locale info.
      Sys.setlocale("LC_ALL", lc.all)
      Sys.setlocale("LC_COLLATE", lc.collate)
      Sys.setlocale("LC_CTYPE", lc.ctype)
      Sys.setlocale("LC_MONETARY", lc.monetary)
      Sys.setlocale("LC_NUMERIC", lc.numeric)
      Sys.setlocale("LC_TIME", lc.time);
    })
  }

  tf <- NULL
  res <- NULL
  if (stringr::str_detect(file, "^https://") ||
      stringr::str_detect(file, "^http://") ||
      stringr::str_detect(file, "^ftp://")) {

    # Download the remote parquet file to the local temp file.
    tf <- tempfile()
    # Remove on exit.
    on.exit(unlink(tf))
    # mode="wb" for binary download
    utils::download.file(file, tf, mode = "wb")
    # Read the local parquet file.
    res <- read_parquet_internal(tf)

  } else {
    res <- read_parquet_internal(file)
  }
  res
}


# Wrapper around arrow::read_parquet to work around #22609 by applying group_by column stored in the parquet file
# as one of the class names by the above _tam_write_parquet_internal.
read_parquet_internal <- function(filepath) {
  res <- arrow::read_parquet(filepath)
  if (stringr::str_starts(class(res)[1], '...grouped_by_')) {
    group_col <- stringr::str_remove(class(res)[1], '^\\.\\.\\.grouped_by_')
    res <- res %>% dplyr::group_by(!!rlang::sym(group_col)) # applying group_by seems to remove the ...grouped_by_... class.
  }
  res
}


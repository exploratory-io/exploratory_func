#' API to download remote data file (excel, csv) from Google Cloud Storage and cache it if necessary
#' it uses tempfile https://stat.ethz.ch/R-manual/R-devel/library/base/html/tempfile.html
#' and a R variable with name of hashed bucket, file are  assigned to the path given by tempfile.
downloadDataFileFromGoogleCloudStorage <- function(bucket, file){
  token <- exploratory:::getGoogleTokenForCloudStorage()
  googleAuthR::gar_auth(token = token, skip_fetch = TRUE)

  shouldCacheFile <- getOption("tam.should.cache.datafile")
  filepath <- NULL
  hash <- digest::digest(stringr::str_c(bucket, file, sep = ":"), "md5", serialize = FALSE)
  tryCatch({
    filepath <- getDownloadedFilePath(hash)
  }, error = function(e){
    # if filePath hash is not set as global variable yet, it raises error that says object not found
    # which can be ignored
    filepath <- NULL
  })
  # Check if cached excel/csv exists for the file path
  if (!is.null(shouldCacheFile) && isTRUE(shouldCacheFile) && !is.null(filepath)) {
    filepath
  } else {
    ext <- stringr::str_to_lower(tools::file_ext(file))
    tmp <- tempfile(fileext = stringr::str_c(".", ext))

    # In case of using Rserve on linux, somehow it doesn't create a temporary
    # directory specified by tempdir() which is used as a part of temp file
    # path generated by tempfile(). So if you try to use that temp file path,
    # dump some data into it for example, it will fail because no such path
    # found. This function fails with the same reason at download.file below.
    #
    # It works fine from the R command line on linux, and it works
    # fine all the time on Mac and Windows regardless Rserv or not.
    #
    # The following command is harmless even if you have the directory already.
    # http://stackoverflow.com/questions/4216753/check-existence-of-directory-and-create-if-doesnt-exist
    dir.create(tempdir(), showWarnings = FALSE)

    # download file to temporary location
    googleCloudStorageR::gcs_get_object(file, bucket = bucket, saveToDisk = tmp)
    # cache file
    if(!is.null(shouldCacheFile) && isTRUE(shouldCacheFile)){
      setDownloadedFilePath(hash, tmp)
    }
    tmp
  }
}

#' Fixed wrapper for gcs_list_objects that handles pagination correctly
#' This function fixes the http_400 Next page token not valid error by
#' properly handling pageToken (not sending empty string in first request)
#' @param bucket bucket containing the objects
#' @param detail Set level of detail ("summary", "more", or "full")
#' @param prefix Filter results to objects whose names begin with this prefix
#' @param delimiter Use to list objects like a directory listing
#' @param versions If TRUE, lists all versions of an object
#' @return A data.frame of the objects
#' @export
#' @keywords internal
gcs_list_objects_fixed <- function(bucket,
                                   detail = c("summary", "more", "full"),
                                   prefix = NULL,
                                   delimiter = NULL,
                                   versions = FALSE) {
  if (!requireNamespace("googleCloudStorageR", quietly = TRUE)) {
    stop("package googleCloudStorageR must be installed.")
  }
  if (!requireNamespace("googleAuthR", quietly = TRUE)) {
    stop("package googleAuthR must be installed.")
  }

  detail <- match.arg(detail)

  # Helper function to safely access internal functions
  safe_get_internal <- function(pkg, func_name, fallback = NULL) {
    if (exists(func_name, envir = asNamespace(pkg), mode = "function")) {
      return(get(func_name, envir = asNamespace(pkg), mode = "function"))
    } else if (!is.null(fallback)) {
      return(fallback)
    } else {
      return(NULL)
    }
  }

  # Get bucket name using internal function from googleCloudStorageR
  as_bucket_name_func <- safe_get_internal("googleCloudStorageR", "as.bucket_name")
  if (!is.null(as_bucket_name_func)) {
    bucket <- as_bucket_name_func(bucket)
  } else {
    # Fallback: just use the bucket name as-is
    bucket <- as.character(bucket)
  }

  # Get storage host - check environment variable first, then default
  # This replicates the logic of get_storage_host()
  storage_host <- Sys.getenv("STORAGE_EMULATOR_HOST", unset = "")
  if (storage_host == "") {
    # Default to standard Google Cloud Storage API host
    storage_host <- "https://storage.googleapis.com"
  }

  # Build base parameters (without pageToken)
  base_pars <- list(
    prefix = prefix,
    delimiter = delimiter,
    versions = versions
  )
  # Remove NULL values
  base_pars <- base_pars[!vapply(base_pars, is.null, logical(1))]

  # API endpoint
  endpoint <- sprintf("%s/storage/v1/b/%s/o", storage_host, bucket)

  # Function to parse response (replicating parse_lo logic)
  parse_response <- function(x) {
    nextPageToken <- x$nextPageToken
    if (is.null(x$items) || length(x$items) == 0) {
      result <- data.frame()
      attr(result, "nextPageToken") <- nextPageToken
      attr(result, "prefixes") <- x$prefixes
      return(result)
    }
    
    # Convert items list to data.frame
    # If items is already a data.frame (from jsonlite), use it directly
    # Otherwise convert list to data.frame
    if (is.data.frame(x$items)) {
      items <- x$items
    } else if (is.list(x$items) && length(x$items) > 0) {
      # Convert list of lists to data.frame
      # Use dplyr::bind_rows if available, otherwise use a safer approach
      if (requireNamespace("dplyr", quietly = TRUE)) {
        items <- dplyr::bind_rows(lapply(x$items, function(item) {
          as.data.frame(item, stringsAsFactors = FALSE)
        }))
      } else {
        # Fallback: convert each item to data.frame and combine
        item_dfs <- lapply(x$items, function(item) {
          as.data.frame(item, stringsAsFactors = FALSE)
        })
        # Get all unique column names
        all_cols <- unique(unlist(lapply(item_dfs, colnames)))
        # Add missing columns to each data.frame
        item_dfs <- lapply(item_dfs, function(df) {
          missing_cols <- setdiff(all_cols, colnames(df))
          for (col in missing_cols) {
            df[[col]] <- NA
          }
          df[, all_cols, drop = FALSE]
        })
        items <- do.call(rbind, item_dfs)
      }
    } else {
      items <- data.frame()
    }
    
    if (nrow(items) == 0) {
      result <- data.frame()
      attr(result, "nextPageToken") <- nextPageToken
      attr(result, "prefixes") <- x$prefixes
      return(result)
    }

    # Convert timestamps if columns exist
    # Helper function to convert Google timestamp to R POSIXct
    timestamp_to_r <- function(ts) {
      if (is.null(ts) || length(ts) == 0) return(NULL)
      # Google timestamps are in RFC 3339 format: "2023-01-01T12:00:00.000Z"
      # Try to parse as POSIXct
      tryCatch({
        as.POSIXct(ts, format = "%Y-%m-%dT%H:%M:%OS", tz = "UTC")
      }, error = function(e) {
        # Fallback: try lubridate if available
        if (requireNamespace("lubridate", quietly = TRUE)) {
          lubridate::ymd_hms(ts, tz = "UTC")
        } else {
          ts  # Return as-is if parsing fails
        }
      })
    }
    
    if ("timeCreated" %in% names(items) && !is.null(items$timeCreated)) {
      items$timeCreated <- timestamp_to_r(items$timeCreated)
    }
    if ("updated" %in% names(items) && !is.null(items$updated)) {
      items$updated <- timestamp_to_r(items$updated)
    }

    # Remove kind column if it exists
    if ("kind" %in% names(items)) {
      items$kind <- NULL
    }

    # Add size_bytes column and format size
    if ("size" %in% names(items)) {
      items$size_bytes <- as.numeric(items$size)
      # Helper function to format object size (replicates format_object_size)
      # This handles a single value (used with vapply)
      format_object_size <- function(bytes, units = "auto") {
        if (is.na(bytes) || bytes == 0) return("0 B")
        if (units == "auto") {
          unit_names <- c("B", "KB", "MB", "GB", "TB", "PB")
          k <- 1024
          sizes <- bytes / (k^(0:(length(unit_names) - 1)))
          idx <- max(which(sizes >= 1))
          return(paste(round(sizes[idx], 2), unit_names[idx]))
        } else {
          return(paste(bytes, units))
        }
      }
      items$size <- vapply(as.numeric(items$size),
                          function(sz) format_object_size(sz, "auto"),
                          character(1))
    }

    # Add extra columns for composite objects if they don't exist
    if (!"componentCount" %in% names(items)) {
      items$componentCount <- NA
    } else {
      items$componentCount[is.null(items$componentCount)] <- NA
    }
    if (!"contentLanguage" %in% names(items)) {
      items$contentLanguage <- NA
    } else {
      items$contentLanguage[is.null(items$contentLanguage)] <- NA
    }

    # Store nextPageToken and other attributes
    attr(items, "nextPageToken") <- nextPageToken
    attr(items, "prefixes") <- x$prefixes
    if ("metadata" %in% names(items)) {
      attr(items, "metadata") <- items$metadata
    }

    items
  }

  # Function to limit columns based on detail level
  limit_columns <- function(req, detail) {
    if (nrow(req) == 0) {
      return(data.frame())
    }

    out_names <- switch(detail,
      summary = c("name", "size", "updated", "size_bytes"),
      more = c("name", "size", "bucket", "contentType",
               "timeCreated", "updated", "storageClass",
               "size_bytes"),
      full = TRUE
    )

    if (is.logical(out_names) && out_names) {
      req
    } else {
      req[, out_names, drop = FALSE]
    }
  }

  # Manual pagination loop
  all_results <- list()
  page_token <- NULL
  page_num <- 1
  pagination_error <- FALSE

  while (TRUE) {
    # Build parameters for this request
    pars <- base_pars
    # Only include pageToken if we have a valid non-empty token
    # This is the key fix: we NEVER send an empty string, only valid tokens from API responses
    if (!is.null(page_token) && page_token != "") {
      pars$pageToken <- page_token
    }

    # Create API generator for this request
    lo <- googleAuthR::gar_api_generator(
      endpoint,
      pars_args = pars,
      data_parse_function = parse_response,
      checkTrailingSlash = FALSE
    )

    # Make the API call
    api_error <- NULL
    tryCatch({
      response <- lo()
    }, error = function(e) {
      # Check if it's the specific pagination error we're trying to fix
      if (grepl("http_400.*Next page token not valid", e$message, ignore.case = TRUE)) {
        # If this is not the first page, we got an invalid token from the API
        # This shouldn't happen with our fix, but if it does, we'll stop gracefully
        if (page_num > 1) {
          warning("Received invalid pageToken from API response. This may indicate the token expired or was corrupted. Returning results collected so far.")
          pagination_error <<- TRUE
        } else {
          # First page with invalid token - this is the original bug we're fixing
          stop("Pagination error on first request. This should not happen with the fixed implementation. Error: ", e$message)
        }
      } else {
        api_error <<- e
      }
    })
    
    # If we hit a pagination error on a subsequent page, break and return what we have
    if (pagination_error) {
      break
    }
    
    # If there was another API error, stop
    if (!is.null(api_error)) {
      stop("Error calling Google Cloud Storage API: ", api_error$message)
    }
    
    # If response is NULL (shouldn't happen, but safety check)
    if (is.null(response)) {
      break
    }

    # Check if we got any results and add them
    if (is.data.frame(response) && nrow(response) > 0) {
      all_results[[page_num]] <- response
    } else if (is.data.frame(response) && nrow(response) == 0 && page_num == 1) {
      # First page returned empty - return empty data.frame
      return(data.frame())
    }

    # Get next page token from response
    next_token <- attr(response, "nextPageToken")

    # If no next token, we're done
    if (is.null(next_token) || next_token == "") {
      break
    }

    # Validate that the token is a non-empty string before using it
    # This prevents using corrupted or invalid tokens
    if (!is.character(next_token) || nchar(next_token) == 0) {
      # Invalid token format - stop pagination to avoid errors
      break
    }

    # Set token for next iteration
    page_token <- next_token
    page_num <- page_num + 1
    
    # Safety check: prevent infinite loops (max 1000 pages)
    if (page_num > 1000) {
      warning("Reached maximum page limit (1000). Stopping pagination.")
      break
    }
  }

  # Combine all results
  if (length(all_results) == 0) {
    return(data.frame())
  }

  # Use dplyr::bind_rows to combine - handles different columns gracefully
  # This is safer than rbind when pages might have different column structures
  if (requireNamespace("dplyr", quietly = TRUE)) {
    combined <- dplyr::bind_rows(all_results)
  } else {
    # Fallback: try to ensure all data.frames have the same columns
    # Get all unique column names
    all_cols <- unique(unlist(lapply(all_results, colnames)))
    # Add missing columns to each data.frame
    all_results <- lapply(all_results, function(df) {
      missing_cols <- setdiff(all_cols, colnames(df))
      for (col in missing_cols) {
        df[[col]] <- NA
      }
      # Reorder columns to match
      df[, all_cols, drop = FALSE]
    })
    combined <- do.call(rbind, all_results)
  }

  # Limit columns based on detail level
  limit_columns(combined, detail = detail)
}

#' API to list items inside a Google Cloud Storage Bucket.
#' @param bucket
#' @param prefix
#' @param delimiter
#' @export
listItemsInGoogleCloudStorageBucket <- function(bucket, prefix, delimiter){
  # set token before calling googleCloudStorageR API.
  token <- exploratory:::getGoogleTokenForCloudStorage()
  googleAuthR::gar_auth(token = token, skip_fetch = TRUE)

  # Use the fixed wrapper function instead of the buggy gcs_list_objects
  gcs_list_objects_fixed(bucket = bucket, detail = "more", prefix = prefix, delimiter = delimiter)
}


#' API to clear Google Cloud Storage cache file
#' @param bucket
#' @param file
#' @export
clearGoogleCloudStorageCacheFile <- function(bucket, file){
  options(tam.should.cache.datafile = FALSE)
  hash <- digest::digest(stringr::str_c(bucket, file, sep = ":"), "md5", serialize = FALSE)
  tryCatch({
    filepath <- eval(as.name(hash))
    do.call(rm, c(as.name(hash)),envir = .GlobalEnv)
    unlink(filepath)
  }, error = function(e){
  })
}

#'Wrapper for readr::guess_encoding to support Google Cloud Storage csv file
#'@param bucket
#'@param file
#'@param n_max
#'@param threshold
#'@export
guessFileEncodingForGoogleCloudStorageFile <- function(bucket, file, n_max = 1e4, threshold = 0.20){
  loadNamespace("readr")
  filePath <- downloadDataFileFromGoogleCloudStorage(bucket = bucket, file = file)
  readr::guess_encoding(filePath, n_max, threshold)

}


#'API that imports a CSV file from Google Cloud Storage.
#'@export
getCSVFileFromGoogleCloudStorage <- function(file, bucket, delim, quote = '"',
                             escape_backslash = FALSE, escape_double = TRUE,
                             col_names = TRUE, col_types = NULL,
                             locale = readr::default_locale(),
                             na = c("", "NA"), quoted_na = TRUE,
                             comment = "", trim_ws = FALSE,
                             skip = 0, n_max = Inf, guess_max = min(1000, n_max),
                             progress = interactive()) {
  tryCatch({
    filePath <- downloadDataFileFromGoogleCloudStorage(bucket = bucket, file = file)
  }, error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket, file)), ' :: There is no file in the Google Cloud Storage bucket that matches with the name.'))
    }
    else {
      stop(e)
    }
  })
  exploratory::read_delim_file(filePath, delim = delim, quote = quote,
                               escape_backslash = escape_backslash, escape_double = escape_double,
                               col_names = col_names, col_types = col_types,
                               locale = locale,
                               na = na, quoted_na = quoted_na,
                               comment = comment, trim_ws = trim_ws,
                               skip = skip, n_max = n_max, guess_max = guess_max,
                               progress = progress)
}

#'API that imports multiple same structure CSV files and merge it to a single data frame
#'
#'For col_types parameter, by default it forces character to make sure that merging the CSV based data frames doesn't error out due to column data types mismatch.
# Once the data frames merging is done, readr::type_convert is called from Exploratory Desktop to restore the column data types.

#'@export
getCSVFilesFromGoogleCloudStorage <- function(files, bucket, for_preview = FALSE, delim, quote = '"',
                              escape_backslash = FALSE, escape_double = TRUE,
                              col_names = TRUE, col_types = readr::cols(.default = readr::col_character()),
                              locale = readr::default_locale(),
                              na = c("", "NA"), quoted_na = TRUE,
                              comment = "", trim_ws = FALSE,
                              skip = 0, n_max = Inf, guess_max = min(1000, n_max),
                              progress = interactive()) {
  # for preview mode, just use the first file.
  if (for_preview & length(files) > 0) {
    files <- files[1]
  }
  # set name to the files so that it can be used for the "id" column created by purrr:map_dfr.
  files <- setNames(as.list(files), files)
  df <- purrr::map_dfr(files, exploratory::getCSVFileFromGoogleCloudStorage, bucket = bucket, delim = delim, quote = quote,
                       escape_backslash = escape_backslash, escape_double = escape_double,
                       col_names = col_names, col_types = col_types,
                       locale = locale,
                       na = na, quoted_na = quoted_na,
                       comment = comment, trim_ws = trim_ws,
                       skip = skip, n_max = n_max, guess_max = guess_max,
                       progress = progress, .id = "exp.file.id") %>% mutate(exp.file.id = basename(exp.file.id))  # extract file name from full path with basename and create file.id column.
  id_col <- avoid_conflict(colnames(df), "id")
  # copy internal exp.file.id to the id column.
  df[[id_col]] <- df[["exp.file.id"]]
  # drop internal column and move the id column to the very beginning.
  df %>% dplyr::select(!!rlang::sym(id_col), dplyr::everything(), -exp.file.id)
}

#'API that search files by search keyword then imports multiple same structure CSV files and merge it to a single data frame
#'
#'For col_types parameter, by default it forces character to make sure that merging the CSV based data frames doesn't error out due to column data types mismatch.
# Once the data frames merging is done, readr::type_convert is called from Exploratory Desktop to restore the column data types.

#'@export
searchAndGetCSVFilesFromGoogleCloudStorage <- function(bucket = "", folder = "", search_keyword = "", for_preview = FALSE, delim, quote = '"',
                                       escape_backslash = FALSE, escape_double = TRUE,
                                       col_names = TRUE, col_types = readr::cols(.default = readr::col_character()),
                                       locale = readr::default_locale(),
                                       na = c("", "NA"), quoted_na = TRUE,
                                       comment = "", trim_ws = FALSE,
                                       skip = 0, n_max = Inf, guess_max = min(1000, n_max),
                                       progress = interactive()) {
  # set token before calling googleCloudStorageR API.
  token <- exploratory:::getGoogleTokenForCloudStorage()
  googleAuthR::gar_auth(token = token, skip_fetch = TRUE)

  # search condition is case insensitive. (ref: https://www.regular-expressions.info/modifiers.html, https://stackoverflow.com/questions/5671719/case-insensitive-search-of-a-list-in-r)
  tryCatch({
    files <- gcs_list_objects_fixed(bucket = bucket, detail = "more", prefix = folder, delimiter = "/") %>%
      filter(str_detect(name, stringr::str_c("(?i)", search_keyword)))
  }, error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket)), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.'))
    }
    else {
      stop(e)
    }
  })
  if (nrow(files) == 0) {
    stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(bucket), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.')) # TODO: escape bucket name.
  }
  getCSVFilesFromGoogleCloudStorage(files = files$name, bucket = bucket, for_preview = for_preview, delim = delim, quote = quote,
                    col_names = col_names, col_types = col_types, locale = locale, na = na, quoted_na = quoted_na, comment = comment, trim_ws = trim_ws,
                    skip = skip, n_max = n_max, guess_max = guess_max, progress = progress)

}


#'API that imports a Excel file from Google Cloud Storage.
#'@export
getExcelFileFromGoogleCloudStorage <- function(file, bucket, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip = 0, trim_ws = TRUE, n_max = Inf, use_readxl = NULL, detectDates = FALSE, skipEmptyRows = FALSE, skipEmptyCols = FALSE, check.names = FALSE, tzone = NULL, convertDataTypeToChar = FALSE, ...) {
  tryCatch({
    filePath <- downloadDataFileFromGoogleCloudStorage(bucket = bucket, file = file)
  }, error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket)), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.'))
    }
    else {
      stop(e)
    }
  })
  exploratory::read_excel_file(path = filePath, sheet = sheet, col_names = col_names, col_types = col_types, na = na, skip = skip, trim_ws = trim_ws, n_max = n_max, use_readxl = use_readxl, detectDates = detectDates, skipEmptyRows =  skipEmptyRows, skipEmptyCols = skipEmptyCols, check.names = check.names, tzone = tzone, convertDataTypeToChar = convertDataTypeToChar, ...)
}

#'API that search files by search keyword then imports multiple same structure Excel files and merge it to a single data frame
#'
#'For col_types parameter, by default it forces character to make sure that merging the Excel based data frames doesn't error out due to column data types mismatch.
# Once the data frames merging is done, readr::type_convert is called from Exploratory Desktop to restore the column data types.

#'@export
searchAndGetExcelFilesFromGoogleCloudStorage <- function(bucket = '', folder = '', search_keyword, for_preview = FALSE, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip = 0, trim_ws = TRUE, n_max = Inf, use_readxl = NULL, detectDates = FALSE, skipEmptyRows = FALSE, skipEmptyCols = FALSE, check.names = FALSE, tzone = NULL, convertDataTypeToChar = TRUE, ...){
  # set token before calling googleCloudStorageR API.
  token <- exploratory:::getGoogleTokenForCloudStorage()
  googleAuthR::gar_auth(token = token, skip_fetch = TRUE)

  # search condition is case insensitive. (ref: https://www.regular-expressions.info/modifiers.html, https://stackoverflow.com/questions/5671719/case-insensitive-search-of-a-list-in-r)
  tryCatch({
    files <- gcs_list_objects_fixed(bucket = bucket, detail = "more", prefix = folder, delimiter = "/") %>%
      filter(str_detect(name, stringr::str_c("(?i)", search_keyword)))
  }, error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket)), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.'))
    }
    else {
      stop(e)
    }
  })
  if (nrow(files) == 0) {
    stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket)), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.'))
  }
  exploratory::getExcelFilesFromGoogleCloudStorage(files = files$name, bucket = bucket, for_preview = for_preview, sheet = sheet,
                                   col_names = col_names, col_types = col_types, na = na, skip = skip, trim_ws = trim_ws, n_max = n_max,
                                   use_readxl = use_readxl, detectDates = detectDates, skipEmptyRows = skipEmptyRows, skipEmptyCols = skipEmptyCols,
                                   check.names = check.names, tzone = tzone, convertDataTypeToChar = convertDataTypeToChar, ...)
}

#'API that imports multiple Excel files from AWS S3.
#'@export
getExcelFilesFromGoogleCloudStorage <- function(files, bucket, for_preview = FALSE, sheet = 1, col_names = TRUE, col_types = NULL, na = "", skip = 0, trim_ws = TRUE, n_max = Inf, use_readxl = NULL, detectDates = FALSE, skipEmptyRows = FALSE, skipEmptyCols = FALSE, check.names = FALSE, tzone = NULL, convertDataTypeToChar = TRUE, ...) {
  # for preview mode, just use the first file.
  if (for_preview & length(files) > 0) {
    files <- files[1]
  }
  # set name to the files so that it can be used for the "id" column created by purrr:map_dfr.
  files <- setNames(as.list(files), files)
  df <- purrr::map_dfr(files, exploratory::getExcelFileFromGoogleCloudStorage,  bucket = bucket, sheet = sheet,
                       col_names = col_names, col_types = col_types, na = na, skip = skip, trim_ws = trim_ws, n_max = n_max, use_readxl = use_readxl,
                       detectDates = detectDates, skipEmptyRows =  skipEmptyRows, skipEmptyCols = skipEmptyCols, check.names = check.names,
                       tzone = tzone, convertDataTypeToChar = convertDataTypeToChar, .id = "exp.file.id") %>% mutate(exp.file.id = basename(exp.file.id))  # extract file name from full path with basename and create file.id column.
  id_col <- avoid_conflict(colnames(df), "id")
  # copy internal exp.file.id to the id column.
  df[[id_col]] <- df[["exp.file.id"]]
  # drop internal column and move the id column to the very beginning.
  df %>% dplyr::select(!!rlang::sym(id_col), dplyr::everything(), -exp.file.id)
}

#'Wrapper for readxl::excel_sheets to support AWS S3 Excel file
#'@export
getExcelSheetsFromGoogleCloudStorageExcelFile <- function(file, bucket){
  filePath <- downloadDataFileFromGoogleCloudStorage(bucket = bucket, file = file)
  readxl::excel_sheets(filePath)
}

#'API that imports a Parquet file from Google Cloud Storage.
#'@export
getParquetFileFromGoogleCloudStorage <- function(file, bucket, col_select = NULL) {
  tryCatch({
    filePath <- downloadDataFileFromGoogleCloudStorage(bucket = bucket, file = file)
  },  error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket, file)), ' :: There is no file in the Google Cloud Storage bucket that matches with the name.'))
    }
    else {
      stop(e)
    }
  })
  exploratory::read_parquet_file(filePath, col_select = col_select)
}

#'API that imports multiple same structure Parquet files and merge it to a single data frame
#'
#'@export
getParquetFilesFromGoogleCloudStorage <- function(files, bucket, for_preview = FALSE, col_select = NULL) {
  # for preview mode, just use the first file.
  if (for_preview & length(files) > 0) {
    files <- files[1]
  }
  # set name to the files so that it can be used for the "id" column created by purrr:map_dfr.
  files <- setNames(as.list(files), files)
  df <- purrr::map_dfr(files, exploratory::getParquetFileFromGoogleCloudStorage, bucket = bucket, col_select = col_select, .id = "exp.file.id") %>% mutate(exp.file.id = basename(exp.file.id))  # extract file name from full path with basename and create file.id column.
  id_col <- avoid_conflict(colnames(df), "id")
  # copy internal exp.file.id to the id column.
  df[[id_col]] <- df[["exp.file.id"]]
  # drop internal column and move the id column to the very beginning.
  df %>% dplyr::select(!!rlang::sym(id_col), dplyr::everything(), -exp.file.id)
}

#'API that search files by search keyword then imports multiple same structure Parquet files and merge it to a single data frame
#'
#'@export
searchAndGetParquetFilesFromGoogleCloudStorage <- function(bucket = '', folder = '', search_keyword, for_preview = FALSE, col_select = NULL) {
  # set token before calling googleCloudStorageR API.
  token <- exploratory:::getGoogleTokenForCloudStorage()
  googleAuthR::gar_auth(token = token, skip_fetch = TRUE)

  # search condition is case insensitive. (ref: https://www.regular-expressions.info/modifiers.html, https://stackoverflow.com/questions/5671719/case-insensitive-search-of-a-list-in-r)
  tryCatch({
    files <- gcs_list_objects_fixed(bucket = bucket, detail = "more", prefix = folder, delimiter = "/") %>%
      filter(str_detect(name, stringr::str_c("(?i)", search_keyword)))
  }, error = function(e) {
    if (stringr::str_detect(e$message, "http_404 The specified bucket does not exist")) {
      # Looking for error that looks like "http_404 The specified bucket does not exist.".
      # This seems to be returned when the bucket itself does not exist.
      stop(paste0('EXP-DATASRC-18 :: ', jsonlite::toJSON(c(bucket)), ' :: The Google Cloud Storage bucket does not exist.'))
    } else if (stringr::str_detect(e$message, "http_404 Unspecified error")) {
      # Looking for error that looks like "http_404 Unspecified error".
      # This seems to be returned when the file does not exist.
      stop(paste0('EXP-DATASRC-19 :: ', jsonlite::toJSON(c(bucket)), ' :: There is no file in the Google Cloud Storage bucket that matches with the file name.'))
    }
    else {
      stop(e)
    }
  })
  if (nrow(files) == 0) {
    stop(paste0('EXP-DATASRC-4 :: ', jsonlite::toJSON(bucket), ' :: There is no file in the AWS S3 bucket that matches with the specified condition.')) # TODO: escape bucket name.
  }
  getParquetFilesFromGoogleCloudStorage(files = files$name, bucket = bucket, for_preview = for_preview, col_select = col_select)

}

